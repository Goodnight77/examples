{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qdrant & Image Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Semantic Search for Accurate Skin Cancer Image Comparison\n",
    "\n",
    "Vector databases are a \"relatively\" new way for interacting with abstract data representations derived from opaque machine learning models -- deep learning architectures being the most common ones. These representations are often called vectors or embeddings and they are a compressed version of the data used to train a machine learning model to accomplish a task (e.g., sentiment analysis, speech recognition, object detection, and many more)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Overview\n",
    "2. Set Up\n",
    "3. Image Embeddings\n",
    "4. Semantic Search\n",
    "7. Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial aims to provide an in-depth walkthrough of how to employ semantic search techniques with image data. In particular, \n",
    "we'll go over an example on how to assist doctors in comparing rare or challenging images of skin cancer with pre-labeled \n",
    "images categorized with different diseases. By leveraging the power of semantic search, medical professionals can enhance \n",
    "their diagnostic capabilities and make more accurate decisions regarding skin cancer diagnosis. That said, you can swap the \n",
    "dataset used in this tutorial and follow along with minimal adjustments to the code. \n",
    "\n",
    "The dataset used can be found in the [Hugging Face Hub](https://huggingface.co/datasets/marmal88/skin_cancer) and you don't \n",
    "need to do anything to download it. Here is a short description of each of the variables available.\n",
    "\n",
    "- `image` - PIL objct of size 600x450\n",
    "- `image_id` - unique id for the image\n",
    "- `lesion_id` - unique id for the type of lesion on the skin of the patient\n",
    "- `dx` - diagnosis given to the patient (e.g., melanocytic_Nevi, melanoma, benign_keratosis-like_lesions, basal_cell_carcinoma, \n",
    "actinic_keratoses, vascular_lesions, dermatofibroma)\n",
    "- `dx_type` - type of diagnosis (e.g., histo, follow_up, consensus, confocal)\n",
    "- `age` - the age of the patients from 5 to 86 (some values are missing)\n",
    "- `sex` - the gender of the patient (female, male, and unknown)\n",
    "- `localization` - location of the spot in the body (e.g., 'lower extremity', 'upper extremity', 'neck', 'face', 'back', \n",
    "'chest', 'ear', 'abdomen', 'scalp', 'hand', 'trunk', 'unknown', 'foot', 'genital', 'acral')\n",
    "\n",
    "By the end of the tutorial, you will be able to extract embeddings from images using transformers and conduct image-to-image semantic search with Qdrant."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Up"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you run any line of code, please make sure you have \n",
    "1. downloaded the data\n",
    "2. created a virtual environment (if not in Google Colab)\n",
    "3. installed the packages below\n",
    "4. started a container with Qdrant\n",
    "\n",
    "```bash\n",
    "# with conda or mamba if you have it installed\n",
    "mamba env create -n my_env python=3.10\n",
    "mamba activate my_env\n",
    "\n",
    "# or with virtualenv\n",
    "python -m venv venv\n",
    "source venv/bin/activate\n",
    "\n",
    "# install packages\n",
    "pip install qdrant-client transformers datasets torch numpy\n",
    "```\n",
    "\n",
    "The open source version of Qdrant is available as a docker image and it can be pulled and run from any machine with docker installed. If you don't have Docker installed in your PC you can follow the instructions in the official documentation [here](https://docs.docker.com/get-docker/). After that, open your terminal start by downloading the image with the following command.\n",
    "\n",
    "```sh\n",
    "docker pull qdrant/qdrant\n",
    "```\n",
    "\n",
    "Next, initialize Qdrant with the following command, and you should be good to go.\n",
    "\n",
    "```sh\n",
    "docker run -p 6333:6333 \\\n",
    "    -v $(pwd)/qdrant_storage:/qdrant/storage \\\n",
    "    qdrant/qdrant\n",
    "```\n",
    "\n",
    "Verify that you are ready to go by importing the following libraries and connecting to Qdrant via its Python client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTImageProcessor, ViTModel\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http import models\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = QdrantClient(host=\"localhost\", port=6333)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_collection = \"image_collection\"\n",
    "client.recreate_collection(\n",
    "    collection_name=my_collection,\n",
    "    vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Image Embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In computer vision systems, vector databases are used to store image features. Image features are vector representations \n",
    "of images that capture their visual content, and they are used to improve the performance of computer vision tasks such \n",
    "as object detection, image classification, and image retrieval.\n",
    "\n",
    "To extract these useful feature representation from our images, we'll use vision transformers (ViT). ViTs are advanced \n",
    "algorithms that enable computers to \"see\" and understand visual information in similar fashion to how humans do. They \n",
    "use a transformer architecture to process images and extract meaningful features from images.\n",
    "\n",
    "To understand how ViTs work, imagine you have a large jigsaw puzzle with many different pieces. To solve the puzzle, \n",
    "you would typically look at the individual pieces, their shapes, and how they fit together to form the full picture. ViTs \n",
    "work in a similar fashion, meaning, instead of looking at the entire image at once, vision transformers break it down \n",
    "into smaller parts called \"patches.\" Each of these patches is like one piece of the puzzle that captures a specific portion \n",
    "of the image, and these patches are then analyzed and processed by the ViTs.\n",
    "\n",
    "By analysing these patches, the ViTs identify important patterns, such as edges, colors, and textures, and combine them \n",
    "to form a coherent understanding of a given image.\n",
    "\n",
    "That said, let's get started using transformer to analyze and interpret our images more effectively.\n",
    "\n",
    "We'll start by reading in the data and examining one sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"marmal88/skin_cancer\", split='train')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[8500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = dataset[8500][\"image\"]\n",
    "image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The image at index 8500, as shown above, is an instance of melanoma, which is a type of skin cancer that starts in the cells called melanocytes. These are responsible for producing a pigment called melanin that gives color to our skin, hair, and eyes. When melanocytes become damaged or mutate, they can start growing and dividing rapidly, forming a cancerous growth known as melanoma. Melanoma often appears as an unusual or changing mole, spot, or growth on the skin, and it can be caused by excessive exposure to ultraviolet (UV) radiation from the sun or tanning beds, as well as genetic factors. If detected early, melanoma can usually be treated successfully, but if left untreated, it can spread to other parts of the body and become more difficult to treat.\n",
    "\n",
    "Because Melanoma can often be difficult to detect, and we want to empower doctors with the ability to compare and contrast cases that are difficult to classify without invasive procedures (i.e., by taking a sample of the skin of the patient).\n",
    "\n",
    "In order to search through the images and provide the most similar ones to the doctors, we'll need to download a pre-trained model that will help us extract the embedding layer from our dataset. We'll do this using the transformers library and Facebook's Dino model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "processor = ViTImageProcessor.from_pretrained('facebook/dino-vitb16')\n",
    "model = ViTModel.from_pretrained('facebook/dino-vitb16').to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's process the instance of melanoma we selected earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images=image, return_tensors=\"pt\")\n",
    "inputs['pixel_values'].shape, inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_embedding = model(**inputs).last_hidden_state\n",
    "one_embedding.shape, one_embedding[0, 0, :20]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, what we get back from our preprocessing function is a multi-dimensional tensor represented \n",
    "as [`batch_size`, `channels`, `rows`, `columns`]. The `batch_size` is the amount of samples passed through our \n",
    "feature extractor and the channels are represent the red, green, and blue hues of the image. Lastly, the rows and \n",
    "columns, which can also be thought of as dimensions, represent the width and height of the image, and this \n",
    "4-dimensional representation is the input our model expects. In return, it provides us with a tensor \n",
    "of [`batch_size`, `patches`, `dimensions`], and what's left for us to do is to choose a pooling method \n",
    "for our embedding since it is not feasible to use 197 embedding vectors when one would suffice. For the final step,\n",
    "we'll use mean pooling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_embedding.mean(dim=1).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a function with the process we just walked through above and map it to our dataset to get an \n",
    "embedding vector for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(batch):\n",
    "    inputs = processor(images=batch['image'], return_tensors=\"pt\").to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs).last_hidden_state.mean(dim=1).cpu().numpy()\n",
    "    batch['embeddings'] = outputs\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(get_embeddings, batched=True, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we now have an embedding for each of the images in our dataset. \n",
    "\n",
    "We'll now save the vector of embeddings as a NumPy array so the we don't have to run it again later, and then \n",
    "what we want to do is to create a payload for the metadata about each of our images. We can accomplish \n",
    "this by converting the rest of the columns we didn't use before into a JSON object for each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"vectors\", np.array(dataset['embeddings']), allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = dataset.select_columns([\n",
    "    'dx', 'dx_type', 'age', 'sex', 'localization'\n",
    "]).to_pandas().fillna({\"age\": 0}).to_dict(orient=\"records\")\n",
    "payload[:3]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the cell above we use `.fillna({\"age\": 0})`, that is because there are several missing values in the `age` column. Because \n",
    "we don't want to assume the age of patient, we'll leave this number as 0. It is also important to note that, at the time of writing, \n",
    "Qdrant will not take in NumPy `NaN`s but rather `None` values only.\n",
    "\n",
    "To make sure each image has an explicit id inside Qdrant, we'll create a new column with a range of numbers equivalent to the rows in \n",
    "our dataset. In addition, we'll load the embeddings we just saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = list(range(dataset.num_rows))\n",
    "embeddings = np.load(\"vecs.npy\").tolist()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to upsert the ID of "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "\n",
    "for i in range(0, dataset.num_rows, batch_size):\n",
    "\n",
    "    low_idx = min(i+batch_size, dataset.num_rows)\n",
    "\n",
    "    batch_of_ids = ids[i: low_idx]\n",
    "    batch_of_embs = embeddings[i: low_idx]\n",
    "    batch_of_payloads = payload[i: low_idx]\n",
    "\n",
    "    client.upsert(\n",
    "        collection_name=my_collection,\n",
    "        points=models.Batch(\n",
    "            ids=batch_of_ids,\n",
    "            vectors=batch_of_embs,\n",
    "            payloads=batch_of_payloads\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.count(\n",
    "    collection_name=my_collection, \n",
    "    exact=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[7000]   #.select_columns(\"image\").cast_column(\"image\", Image(decode=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.filter(\n",
    "    lambda x: x == \"ISIC_0031944\", input_columns=\"image_id\"\n",
    ").select_columns(\"image\")[0]['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0][\"image\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.scroll()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vdb_audio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
