{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0579ba58",
   "metadata": {},
   "source": [
    "## Blog-Reading Chatbot with Qdrant, GPT-4o and LangChain\n",
    "\n",
    "This notebook demonstrates how to use LangChain and Qdrant to build a RAG model that processes blog data. \n",
    "The RAG model is a retrieval-augmented model that uses a retriever to find relevant documents and a generator to generate answers. \n",
    "The retriever is built using Qdrant, an open-source vector search engine.\n",
    "For the generator, we are using GPT-4o, OpenAI's most recent language model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cb53cd",
   "metadata": {},
   "source": [
    "## Implementation\n",
    "\n",
    "You can install the required libraries using the following command - get Poetry from [here](https://python-poetry.org/docs/):\n",
    "\n",
    "```bash\n",
    "poetry install\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941725fd-c049-4da7-a442-d3709f47400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f676dd5-0362-4b17-94c2-4168fa092d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef03924",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91af3deb-b596-4280-bfd5-5561ee3d7601",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e884648",
   "metadata": {},
   "source": [
    "## Download and Index\n",
    "\n",
    "You need to first load the blog post contents. To do this, use urllib and BeautifulSoup to load and parse the HTML content. Then index the blog post contents using Qdrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a04288c-fe9b-44ce-b482-e4acda3e2e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40026f58",
   "metadata": {},
   "source": [
    "### Chunk Data Before Indexing\n",
    "\n",
    "The loaded document is over 42k characters long. This is too long to fit in the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n",
    "\n",
    "To handle this, you have to split the document into chunks for embedding and vector storage. This should help you retrieve only the most relevant bits of the blog post at run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2642968",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    documents=splits, embedding=OpenAIEmbeddings(), location=\":memory:\", collection_name=\"lilianweng\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0487f34",
   "metadata": {},
   "source": [
    "Use LangChain's LCEL Runnable protocol to define the chain: 1) context, 2) format_docs, 3) prompt and 4) output parsing. \n",
    "Then, run the chain to get the answer. \n",
    "\n",
    "Download a pre-written prompt from the LangChain hub and run it with the blog post content. \n",
    "The prompt is designed to extract the most relevant information from the blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab0797e-a3db-45d3-b49d-769f99dfef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31079eb2-563b-4f6f-afdb-817302d5e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
