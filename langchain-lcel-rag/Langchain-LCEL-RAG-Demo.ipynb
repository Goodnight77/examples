{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0579ba58",
   "metadata": {},
   "source": [
    "## Langchain x Qdrant: RAG Demo with Web Scraping\n",
    "\n",
    "This notebook demonstrates how to use Langchain and Qdrant to build a RAG model with web scraping. The RAG model is a retrieval-augmented model that uses a retriever to find relevant documents and a generator to generate answers. The retriever is built using Qdrant, an open-source vector search engine, and the generator is GPT-3.5, a language model developed by OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87cb53cd",
   "metadata": {},
   "source": [
    "## Setting Up\n",
    "\n",
    "You can install the required libraries using the following command - get Poetry from [here](https://python-poetry.org/docs/):\n",
    "\n",
    "```bash\n",
    "poetry install\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941725fd-c049-4da7-a442-d3709f47400c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f676dd5-0362-4b17-94c2-4168fa092d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91af3deb-b596-4280-bfd5-5561ee3d7601",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo-0125\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e884648",
   "metadata": {},
   "source": [
    "## Download and Index\n",
    "\n",
    "We need to first load the blog post contents. We use urllib and BeautifulSoup to load and parse the HTML content. We then index the blog post contents using Qdrant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a04288c-fe9b-44ce-b482-e4acda3e2e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40026f58",
   "metadata": {},
   "source": [
    "### Chunking before Indexing\n",
    "\n",
    "Our loaded document is over 42k characters long. This is too long to fit in the context window of many models. Even for those models that could fit the full post in their context window, models can struggle to find information in very long inputs.\n",
    "\n",
    "To handle this weâ€™ll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant bits of the blog post at run time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2642968",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "vectorstore = Qdrant.from_documents(\n",
    "    documents=splits, embedding=OpenAIEmbeddings(), location=\":memory:\", collection_name=\"lilianweng\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0487f34",
   "metadata": {},
   "source": [
    "We Langchain's LCEL Runnable protocol to define the chain: context, format_docs, prompt, output parsing. We then run the chain to get the answer. \n",
    "\n",
    "We download a pre-written prompt from the Langchain hub and run it with the blog post content. The prompt is designed to extract the most relevant information from the blog post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab0797e-a3db-45d3-b49d-769f99dfef43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31079eb2-563b-4f6f-afdb-817302d5e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
